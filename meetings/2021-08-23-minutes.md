# Attribution Reporting API

Aug 23, 2021

Meet link: [https://meet.google.com/jnn-rhxv-nsy](https://meet.google.com/jnn-rhxv-nsy)

Previous meetings: [https://github.com/WICG/conversion-measurement-api/tree/main/meetings](https://github.com/WICG/conversion-measurement-api/tree/main/meetings)

Use Google meet “Raise hand” for queuing.

If you can’t edit the doc during the meeting, try refreshing as permissions may have been updated after you loaded the page.


# Agenda



* Introductions
* Scribe volunteer
* New blogpost: [Introduction to Attribution Reporting (Conversion Measurement)](https://developer.chrome.com/docs/privacy-sandbox/attribution-reporting-introduction/) — An introduction to the API for both technical and non-technical folks (Note: a bit light on the aggregate side).
    * Q: What's missing for this to be a helpful intro guide? Feedback welcome. 
    * Note: it includes a list of use cases, will modify as we collaborate on the [open issue](https://github.com/WICG/conversion-measurement-api/issues/170)
* [Use-case doc](https://docs.google.com/document/d/1vjifVEvujmoscy8qYAZzMYX0SaG8uGmeeehPsIwmAGc/edit) ([#170](https://github.com/WICG/conversion-measurement-api/issues/170))
* Declarative aggregate API ([#194](https://github.com/WICG/conversion-measurement-api/issues/194))
* Keys.json format ([#179](https://github.com/WICG/conversion-measurement-api/issues/179))


# Attendees — please sign yourself in! 



1. Erik Anderson (Microsoft)
2. Charlie Harrison (Google Chrome)
3. Brain May (dstillery)
4. Alex Turner (Google Chrome)
5. John Delaney (Google Chrome)
6. Badih Ghazi (Google Research)
7. Erik Taubeneck (Facebook)
8. Nicolas Chrysanthos (Criteo)
9. Chris Evans (NextRoll)
10. Brad Lassey (Google Chrome)
11. Maud Nalpas (Google Chrome)
12. Raj Gupta (Captify)
13. Basile Leparmentier (Criteo)
14. Andrew Pascoe (NextRoll)
15. Brendan Riordan-Butterworth (eyeo GmbH)


# Notes

**New blogpost announcement**



* Idea for this post is to be a way for technical folks and less technical folks to onboard with the API. It includes a section on status of the various features within the API, explanation on the privacy mechanisms.
* Reason for sharing: It would be interesting to know whether it is helper, and what is missing.

**Use cases**



* Have a provisional list of use-cases for the API so far. We want this section to evolve and incorporate feedback. Stemmed from issue 170. Lists use-cases and suggestions.
* What do you intend to use the API for?
* Brian May: Thank you for publishing this. It’s helpful for communicating things to other people in the company
* Maud: Feedback welcome. Should be an easier way to onboard which expect you to have some basis already on the API
* Basil: Status of the origin trial means we can try it on our own browser?
* Maud: Some features are available in Chrome. For those features there are two ways to try them out
    * Locally in the browser via command line flags (like browser settings)
    * Origin trials. This activates the API on end users (not locally). First step is to register for an Origin Trial, this gives you a token which you add to your site (tag / header). Turns on the API for end-users.
* Brian: We tend to be one-step removed from any events. Relying on other people to get data back about what’s going on with our campaigns. I think it would be very helpful to third parties that are positioned like us who don’t have direct contact with advertising events to have an explainer for working with first-parties.
* Maud: not on the receiving end of reports?
* Brian: Will put something down in writing and send to you
* Raj Gupta: Question regarding the Origin Trial. I have signed up. The final date is the 15th of September. Can this be extended? Still in discussion with clients
* John Delany: Normally Origin Trials run for a couple of milestones at a time. If we get feedback on people still wanting to experiment, we can definitely factor that in and look into extending the Origin Trial.
* Raj: For view through conversions?
* John: We haven’t published any timeline currently, but can call out a mailing list that includes updates. [attribution-reporting-api-dev@chromium.org](mailto:attribution-reporting-api-dev@chromium.org)
* Raj: Last-click attribution model? This is in experimental phase as well but couldn’t find a link to sign up?
    * Event-level reports for clicks
    * Attribution model: last click
    * How are these different?
* Maud: The distinction makes this confusing. The default is last click. Upcoming feature uses priorities
* Raj: Event-level reports for views, when that goes live. I presume we’ll do both views as well as clicks?
* Maud & Charlie: Yes :)
* Maud: List of use-cases, looking for feedback and thoughts! Please refer to issue #170. Add your thoughts!

**Declarative aggregate API ([#194](https://github.com/WICG/conversion-measurement-api/issues/194))**



* John: follow-up discussion from last meeting. Currently, to use the aggregate reports and generate histogram contributions, you need to have JS running on the advertiser site or the attribution destination. This issue is exploring a different mechanism
* Called out in a number of places that lots of measurement things are HTTP-only, so it would be great to support existing solutions similar to event-level
* Last week we discussed a couple of things about how to specify contributions
* Erik: both imp and conv could declare a set of buckets, browser takes an intersection of them
    * Big downside: combinatorial explosion of buckets! E.g. for 1000 products
* Here is a new proposal (“Generic Matching”), idea is that an impression and conversions declare a set of “partial histogram buckets”, each have a partial bucket ID, ones on conversion side have a value
* Rather than choosing already-made buckets, form buckets from both imp and conversion-side information. Ideally this would be a more compact encoding and let you do more complicated things.
* On both imp / conv you declare some JSON
    * Campaign x country
    * Imp → campaign123 x country=us. Conversion side declare a bucket count

             Campaign123 x country=us counts

* Each partial bucket can declare a set of flags on them. When choosing to pair buckets, browser will match up the flags.
* Lots more work will be done to see how all the use-cases line up
* “Conversion filters” proposal
    * This is really easy in this API
    * Flag is FilterID
    * Automatically filtering all my contributions by this filter ID
* Pros: Good configurability compared to the worklet. No script execution
* Cons: Verbose format, new data model. Requires breaking down existing use-cases into this data model
* Unified mechanism across both HTTP and JS environments
* Erik: THanks for writing this up, making good progress. If you only specify one partial bucket per contribution, how does this avoid the explosion talking about before?
* John: Imagine I want to measure counts for a campaign
    * Impression side → campaign
    * Conversion side → I can just select the productID directly, without enumerating all the product
    * Benefit is you can choose the buckets based on conversion-side information
* Erik: It would match based on the product ID, would i
* John:
    * Campaign x shoes
    * Campaign x backpacks
    * Based on the conversion you can choose
    * You would only need one, you don’t need to list all the others
* Erik: Not following
* John: Buckets by each get added together, missing from this issue
* Brian: What you’re proposing is to collect raw data into buckets that you know about at event-time. At reporting, you figure out how to allocate the data you’ve collected in those buckets under specific reporting flags. SO you don’t have to create raw data buckets spread across different reporting instances.
* John: Yeah, I think something like that is possible. Didn’t follow the last point, can you clarify?
* Brian: Campaign with 100 impressions, get a conversion on backpacks. ALlocate those 100 to backpacks. At reporting time, making a decision where the data goes to. Once I’ve done that, I don’t want to report it again under a different bucket.
* John: Would benefit from talking about use-cases in this issue. This is per impression. Each time I have an impression, I have these buckets. Each time there is a conversion, browser does attribution. Only thing optimizing here is the contributions for a single attributed conversion
* Charlie: This proposal gives us a combination of “concatenate imp key and conversion key” and the proposal from last week where each imp and conv lists all possible valid keys
* Erik: Want to suggest an alternative. Tossing this in my head. Can’t tell if less functional, but simpler to implement. Idea would be to just have partial bucket, where you denote the bucket bitstring, can put wildcards (0/1, *), * always matches what’s on the other side. Impression can specify the bits it cares about matching on, leaves the other ones open. Find an intersection where it’s specified, take the set digit where it’s a wildcard. That should allow you to say on the impression side “here’s the campaign bits I care about, here are some of the bits of the item I care about”, on conversion side “here are some of the bits of the campaign I care about and all of the bits of the item”, can AND that thing together. Think that gives the same flexibility here.
* John: could that complicate how people how to manage their bucket allocations?
    * As is, you don’t need to have a hierarchy of IDs. You could do it that way but it may be difficult to understand how the bucket ID is constructed.
    * It seems reasonable
* Erik: If you leave it as 32 bits, can leave it to ad-tech for how to construct. Have to think about this when you are adding the buckets together as well
* John: Also another idea of having strings of buckets as a different concept
* Charlie:
    * John to update to describe merging logic
    * Erik to add his proposal
    * Everyone to look at it from the developer’s POV :)
* Basile: Just to understand. Idea is to avoid comb. Comb explosion might not be because of imp x conversion matching. Even single-site contributions have large dimensionalities. If you want to add on top of that conversion-side information, it will obviously also explode it more.
* Charlie: This is an API surface conversation, not one necessarily about functionality (though that is still an important aspect wrt JS worklet)
* Erik: Limit to the number of buckets that can be contributed to from any given impression
* John: existing explainer there is a limit. We discussed changing this to just have an L1 budget
* Charlie: We’re still discussing this, we may want to keep that limit and not just go with the L1 limit. Not yet decided. The reason we may want this is that it adds reasonable bounds on how many reports 1 device can send out. The ad tech receives the reports, and the count of reports is “in the clear”. One solution is to send some fake reports, but to get DP on the count of the reports, you need to add it proportional to the number of reports that can be sent out.
* John: Slightly unrelated, but in the current mechanism, there is 1 report per histogram contribution. Is it possible to do this so payloads can contain multiple contributions?
    * Current explainer → 3? Can we package them and solve this
* Charlie: Might break depending on the MPC protocol, DPF won’t work (?) but Prio would
* Erik: can you include 0s / nulls it would work.
* Charlie: Yes that will work
* Erik: Are we assuming the ad-tech server is able to identify where the reports come from (with some sort of fingerprinting)
* Charlie: it depends, there are a number of things available in the clear so you may get some coarse bucketing at the advertiser level already
* Erik: These reports are delayed, so if you wanted to sent them would you delay them more
* Charlie: We could explore modifying the delays, trying to keep the delays low (some feedback on delays in event level API)
* Erik: If you want more contributions, maybe you can afford to have more of them delayed. Moreso limiting the number sent at any time then the total number of contributions
* Charli: Something like that could work where you are bounding across time periods. Delaying even more may result in issues where you are generating reports faster than you can consume them, worth exploring. Revealing counts/number of reports isn’t fully fleshed out, but it is useful to think about this bounding. We don’t have a way to generate fake reports, and this gets complicated with auth tokens etc.
* Erik: Should we think about a cap when thinking about the declarative API -> Yes

**Keys.json format (Alex Turner)**

* Issue is to define the format for serving the public keys to encrypt the payloads. We want to ensure this data cannot be read outside of the aggregation service
* One of the key bits of complexity here is that we want these keys to rotate regularly. If a key is compromised, the impact is limited.
* Discussions here are really all around how to encode / enforce
* Originally we had a bunch of key sets, each one would have an explicit validity period.
* Aim here is to be simpler, flexible, and avoid reinventing the wheel.
* Want to just use HTTP cache
* File would be JSON list of objects, representing the various valid keys and each objects. Each object would be a bas64 encoded key and an ID, then the aggregation endpoint would use cache control headers
* Risk: different clients could have different sets of cached keys at the same time
    * E.g. if a file is updated every day but cached for 2 weeks, could have up to 14 variations in the world, could be a tracking vector
    * To limit this possibility, imagine a user agent could enforce limitations on what kind of caching is allow.
    * E.g. caching only as long as the usual key rotation period.
    * E.g. only allowed to cache for a day if you rotate a key every day
    * Could be combined with monitoring / audits
* Charlie: is it the same as the validity windows in the previous iteration?
* Alex: Limit caching to the update period. It would be OK if you’re specifying 5 keys and every day you take one out and put a new one in. You wouldn’t want to cache them beyond your typical update schedule
* John: with the keyset approach managing the validity periods may not be super simple, and you may never want to take advantage of that format. Keys in the future could be compromised. You pay the cost of overlap, 2 keysets out in the wild at a time or 3, but might be OK
    * It is possible for the browser to add their own mechanism to make sure that all clients have the same key set
    * In that case, the keyset approach is less useful. In that case the only issue is how does the browser make sure that it doesn’t have multiple key sets for itself
    * We should consider this possible future
* Erik: I don’t know if we’ve specified which type of encryption. You might leak some of that side channel information to the ad-tech server as well as the helper nodes
* Charlie: can you post to the issue? Out of time!